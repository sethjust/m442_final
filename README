This project is meant to provide a distributed Python compute service. This
repository currently contains elements of the central distributed object store,
written in C, as well as a python library and rudimentary user client and
runner executables, written in python.

The client is a fairly simple wrapper around the socket RPC described below.
The runner utilizes the client's libraries to interact with a python sandbox.

Running the system requires:
1. Running 1 or more C server instances:
  a. On hostA, run ./server. This will start a server on port 11111.
  b. Run more servers with ./server localport server port, where server:port is
    another server.
2. Setting up one or more runner instances:
  -. Before starting, note that the runner is built around the python execution
    sandbox provided by PyPy (pypy.org). The code was written to work with
    release 1.7, but may work with other versions.
  a. Download the PyPy *source* release and unzip it to a folder called
    pypy_src in the same folder as this project. This is where runner.py will
    look to find the sandbox.
  b. In pypy_src/pypy/translator/goal run "python ./translate.py -O2
    --sandbox". Note that it can be up to twice as fast to use a pre-compiled
    pypy release to run this step. Once the process is over (20-60 mins), run
    "../sandbox/pypy_interact.py ./pypy-c" to test that everything works. If
    so, you should get an interactive python prompt.
  c. Run ./runner.py [<port>|<host> <port>]. At the moment it will repeatedly
    get jobs from the given server and upload their results, then exit once
    none are left.
   
3. Using the client api:
  -. Currently, running the client.py file as a script (i.e. "./client.py
    [<port>|<host> <port>]") will run a test scenario.
  a. In python, import the client module with

      from client import ComputeCloud

    Instances of this class will represent all our communications with a cloud
    instance. To create one, initialize it with the hostname and port of a
    server:

      s = ComputeCloud("localhost", 11111)

    You can now upload data to the cloud. An add operation will return an instance of ComputeCloud.FileObject, whose get() method returns the contents of the file.

      f = s.add_string("name1", "contents")
      g = s.add_file("name2", "./path/to/file")
      print f.get() # prints "contents"

    Adding jobs is similar, but requires more information to be provided: the
    name of the job and its output file, the code to be run, and a list of
    files that might be read (a namespace).  Adding a job returns a pair of
    FileObjects, the first associated with the job object and the second
    associated with the jobs output file. For example:

      code = '''for path in ["/net/name1", "/net/name2"]:
        print open(path, 'r').read()'''
      job, out = s.add_job("jobname", code, "outputname", [f, g])

    The above example hints at how jobs can receive input: they are created
    along with a namespace, listing those files that they might access by name.
    When a job is running, opening the path "/net/<name>" returns a file with
    the contents from the LAST object listed in the namespace whose name
    matches <name>. Notably however, these virtual files are soley a source of
    _input_ for jobs, as any open that is not in read-only mode will fail, and
    potentially terminate the job. Output is only accomplished by printing to
    STDOUT and STDERR. These outputs are logged separately. STDERR output is
    meant for debugging use, and is stored with the original job object after
    execution.  STDOUT output is saved into the output file created along with
    the job.  Notice that this means that only STDOUT output from a job may be
    used as input by other jobs.

The architecture of the object store (or "cloud") is a network of peer servers
with some (as yet undecided) way of knowing about its peers. This code
implements communication between servers via socket connections. Each server
listens on a socket and spawns threads for each connection. Synchronization is
managed by the (threadsafe) sqlite storage database used by each node. Each
connection is a series of two way null-terminated string exchanges of the
following format: first the connecting peer sends "COMMAND{:data}*" and the
server responds with "[ACK|NACK]{:data}*". In both cases the data formatting is
determined by the command sent.  Each string is preceeded by a 6 hexadecimal
digits. The first four characters indicate the length (in bytes, not including
the null terminator) of the message to follow (allowing messages up to 65536
bytes), while the final two are the sum of all the bytes in the message (modulo
16^2).  All clients should send the message "STOP" before closing the socket.
(This is technically unneccessary, as a zero-length message results from the
socket closing, but is considered polite).

The following commands are processed by storage nodes:
  File-related:
    ADD:name:bytes -> ACK:hash -- upload a file
    BADD:name:salt:complete:metadata:bytes -> ACK -- copy a file to a non-primary server
    GET:hash -> ACK:bytes -- get the contents of a file
    UPD:hash:stdout[:stderr] -> ACK -- update job status/results
  Job-related:
    JADD:name:sourcebytes:outputname{:inputhash}* -> ACK:jobhash:outputhash -- add a job
    GETJ -> ACK:sourcebytes:hash:{:inputhash:inputname}*
    HAVJ -> ACK
  Server-related:
    SADD{:host:port:salt}* -> ACK -- add server records
    GETS -> ACK{:host:port:salt}* -- get a server table
    SUCC -> ACK -- notify a server that their successor has gone down

A word on data representation:
  hostnames are strings; dotted quads or dns names
  ports are decimal integers of variable width
  hashes are 8-character (0 padded) hexadecimal integers
  salts are hashes
  names are strings
  filenames are names
  bytes are strings (Base64 encodings of binary files)
  complete is a bit ([0|1])
Servers will make a best-faith effort to interpret these in a reasonable (i.e.
human-intuitive) manner.

Objects are stored in "cloud" of peer servers using a relatively simple
consistent hashing scheme. All stored objects (servers, jobs, and files) are
given an arbitrary-length name and an integer salt, which is used to produce a
32-bit hash value. This is the only use of names at the moment, but the feature
is intended to add user-friendliness to large collections of stored objects.
Objects' hash values determine the storage location of an object, namely the
immediately subsequent server(s) in keyspace. Any server will accept requests
concering any object, but may make connections with other servers before
responding. Note that the salt associated with a server object allows any
machine to have multiple distinct locations in keyspace; this is an intended
feature to aid load-balancing.
Because we are not worrying about multiple server failures, we need only keep
two copies of any piece of data to ensure it is not lost. Thus objects are
stored on two subsequent servers. Note that at the moment there is no guarantee
that these servers are distinct.
This hashing and duplication scheme allows for a simple failover method, which
is best explained by considering an established network of at least four
servers, subsequent to one another in keyspace, and an object O that is stored
on B.
    A ---O- B ----- C ----- D
When host B fails, initially no server is aware of the failure. However, when
some request is made for O, the connection with server B will fail, making the
requesting server aware of the failure. Note that if the first request for O is
sourced from a client, they are expected to attempt contacting other servers if
their attempt to reach B fails. The request will then be forwarded to C, which,
after attempting to get O from its canonical location, will recognize the
failure of B. Note that C will always recognize B's failure after one request,
and so should be responsible for repairing the failure. It does so by copying
all (locally stored) objects with hashes falling between A and B to D, thus
taking over the primary storage capacity for B. To take over secondary storage
for B, C then requests A to send copies of all objects that hash between A and
its predecessor (D, if no other servers exist). Note that there is NO global
communication required for sucessful failover; other servers will not be aware
of the failure until they attempt to make a connection to B.
Adding servers to the network is made slightly harder by the fact that failures
will leave out-of-date information on some servers. However, some one-to-all
communication removes this issue, and allows a new server to come up with
knowledge of only one other. In short, a new server begins with a list of at
least one server and performs the following process:
  1. Request a server list from each known server
  2. For any unknown server returned, request a server list
  3. Repeat step 2 until no new servers are returned
With the expectation that failures are rare this process should end within one
or two repetitions.
